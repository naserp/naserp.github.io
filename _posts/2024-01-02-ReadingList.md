---
title: 'My AI Safety Reading List'
date: 2024-01-02
permalink: /posts/2024/02/ReadingList/
tags:
  - AI Safety
  - SafeGaurd 
---

My AI Safety Reading List: Recent Papers and Blog Posts
As AI keep evolving, making sure it’s safe and ethical is getting more important. I've been reading some interesting papers and blog posts that look into different aspects of AI safety, from protecting human-AI chats to reducing risks in large language models (LLMs). Here's a quick look at what I've been digging into:

* Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations
This paper from Meta AI talks about Llama Guard, a tool to protect human-AI conversations by filtering what goes in and comes out of large language models. The idea is to stop unintended harmful outputs during AI interactions, making these models safer to use.
[Read the paper](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/)

* Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space
This paper explores how some prompts can exploit weak spots in open-source LLMs, using the model's embedding space to bypass safety mechanisms. It highlights the need for strong safety alignment techniques to prevent these kinds of attacks from undermining AI systems.
[Read the paper](https://arxiv.org/pdf/2402.09063)

* Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models
This paper is all about the idea of "self-destructing" models that are harder and more expensive to repurpose for harmful uses. The authors suggest different strategies, like structural and technical safeguards, to protect against the dual-use risks of powerful AI models.
[Read the paper](https://arxiv.org/pdf/2211.14946)

* Reducing Toxicity in Language Models
Lilian Weng’s blog post goes over strategies to reduce toxicity in language models. It covers different methods like training with carefully curated datasets, using post-processing filters, and building ethical considerations right into the model development process.
[Read the blog post](https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/)

* Predictability and Surprise in Large Generative Models
This paper looks at how to balance predictability and surprise in generative models, and how these factors influence the behavior and safety of AI systems. Understanding this balance is key to designing models that are both creative and safe.
[Read the paper](https://arxiv.org/abs/2202.07785)

These reads have given me a deeper understanding of the challenges and solutions in AI safety, and show how important it is to take steps to prevent harmful outcomes in AI systems. Whether you're into AI research, development, or just interested in AI ethics, these resources offer valuable insights into the ongoing efforts to make AI safer for everyone.
